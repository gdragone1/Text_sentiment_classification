{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_type": "code",
    "id": "583548D9EA084827BD202D1555DF675B",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E23BE69553354F7BA080FD457F777DAF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # copy、移动文件\n",
    "# from shutil import copyfile\n",
    "# import shutil\n",
    "# WORK_PATH = '/home/kesci/work/tfrecords_2'\n",
    "# TFRECORDS_PATH = os.path.join(WORK_PATH, 'train')\n",
    "# list_ = [filenames for dirpath, dirnames, filenames in os.walk(TFRECORDS_PATH)]\n",
    "# num = 20\n",
    "# for i in list_[0]:\n",
    "#     # print(i)\n",
    "#     if(num == 0):\n",
    "#         break\n",
    "#     num -= 1\n",
    "#     copyfile(WORK_PATH+'/train/'+i, WORK_PATH+'/train_2/'+i)\n",
    "# os.remove('/home/kesci/work/tfrecords_2/train/train.tfrecords-0092-of-1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "69539F9E696B4243829872A508B045C4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# AUC for a binary classifier\n",
    "def auc(y_true, y_pred):\n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "59146AEB0B5D42AD97775E7580CF7C6F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 85000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 230 # max number of words in a question to use\n",
    "maxlen_1 = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "108B8215EB12457F89BA864046EE5520",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 230)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 230, 300)          25500000  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 230, 128)          140544    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 25,642,625\n",
      "Trainable params: 25,642,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4388875A76B646E9805082FB283F5C57",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# TFRECORDS_PATH = os.path.join(WORK_PATH, 'tfrecords_2')\n",
    "# tfrecords_list = [os.path.join(dirpath, filename) for dirpath, dirnames, filenames in os.walk(TFRECORDS_PATH) \\\n",
    "#                                                         for filename in filenames]\n",
    "# l = len(tfrecords_list)\n",
    "# num = 0\n",
    "# for i in tfrecords_list:\n",
    "#     num += 1\n",
    "#     if num < l:\n",
    "#         shutil.move(i, '/home/kesci/work/tfrecords_2/train/')\n",
    "#     else:\n",
    "#         shutil.move(i, '/home/kesci/work/tfrecords_2/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "44AF6EF8252747DE8B94EE3CA7CA9528",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_PATN = './input'\n",
    "WORK_PATH = './work/tfrecords_2'\n",
    "TFRECORDS_PATH = os.path.join(WORK_PATH, 'train')\n",
    "TFRECORDS_VALPATH = os.path.join(WORK_PATH, 'val')\n",
    "# TFRECORDS_PATH = os.path.join(WORK_PATH, 'tfrecords_2')\n",
    "\n",
    "NUM_SHARDS = 20   # 总共写入多少文件\n",
    "INSTANCES_PER_SHARD = 1000    # 每个文件写入多少数据\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list = tf.train.Int64List(value=[value]))\n",
    "\n",
    "def feature_2(x):\n",
    "    p = list([int(i) for i in x.split(\" \")])\n",
    "    t = set(p)\n",
    "    p.sort()\n",
    "    if(len(p) == len(t)):\n",
    "        return tf.train.Feature(int64_list = tf.train.Int64List(value=[len(p)])),tf.train.Feature(int64_list = tf.train.Int64List(value=[0]))\n",
    "    dic = {}.fromkeys(t,0)\n",
    "    for i in p:\n",
    "        dic[i] += 1\n",
    "    sorted(dic.items(),key = lambda x:x[1],reverse = True)\n",
    "    for i in dic:\n",
    "        return tf.train.Feature(int64_list = tf.train.Int64List(value=[len(p)])), tf.train.Feature(int64_list = tf.train.Int64List(value=[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "188F66DCA225436C905B72871C46F12B",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 交 集 和 差 集 以 及 交 集 的 长 度，差 集 的 长 度\n",
    "import re\n",
    "def trans_list(x):\n",
    "    t = x.strip().replace(',',' ')\n",
    "    x = re.sub(r'\\s+', ' ', t.strip())\n",
    "    if(len(x) == 0):\n",
    "        return []\n",
    "    p = list([int(i.strip()) for i in x.split(' ')])\n",
    "    return p\n",
    "    \n",
    "def differ(x, y):\n",
    "    retA = trans_list(x)\n",
    "    retB = trans_list(y)\n",
    "    # 求差集，在retA中但不在retB中\n",
    "    retC = list(set(retA).difference(set(retB)))\n",
    "    retD = list(set(retA).difference(set(retC)))\n",
    "    return retC,len(retC),retD,len(retD)\n",
    "    \n",
    "def parser(record):\n",
    "    features = tf.parse_single_example(\n",
    "        record,\n",
    "        features = {\n",
    "            'query_id': tf.FixedLenFeature([], tf.int64),\n",
    "            'query_title_id': tf.FixedLenFeature([], tf.int64),\n",
    "            'query_len': tf.FixedLenFeature([], tf.int64),\n",
    "            'title_len': tf.FixedLenFeature([], tf.int64),\n",
    "            'fre_query': tf.FixedLenFeature([], tf.int64),\n",
    "            'fre_title': tf.FixedLenFeature([], tf.int64),\n",
    "            'differQ_T': tf.FixedLenFeature([], tf.string),\n",
    "            'differQ_T_len': tf.FixedLenFeature([], tf.int64),\n",
    "            'unionT_Q': tf.FixedLenFeature([], tf.string),\n",
    "            'unionT_Q_len': tf.FixedLenFeature([], tf.int64),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "      })\n",
    "    return features['query_id'], features['query_title_id'], features['query_len'], features['title_len'], features['fre_query'], features['fre_title'], features['differQ_T'], features['differQ_T_len'], features['unionT_Q'], features['unionT_Q_len'], features['label']\n",
    "    \n",
    "def get_example_object():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_train_files(path, batch_size):\n",
    "    while True:\n",
    "        tfrecords_list = [os.path.join(dirpath, filename) for dirpath, dirnames, filenames in os.walk(path) \\\n",
    "                                                        for filename in filenames]\n",
    "        input_files = tf.placeholder(tf.string)\n",
    "        dataset = tf.data.TFRecordDataset(input_files)\n",
    "        dataset = dataset.map(parser)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        query_id, query_title_id, query_len, title_len, fre_query, fre_title, DifferQ_T, DifferQ_T_len, UnionT_Q, UnionT_Q_len, label = iterator.get_next()\n",
    "        X = []\n",
    "        Y = []\n",
    "        \n",
    "        index = 0\n",
    "        instances_num = 0\n",
    "        tokenizer = Tokenizer(num_words = max_features)\n",
    "        cnt = 0\n",
    "        differ_qt = []\n",
    "        union_tq = []\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(iterator.initializer, feed_dict={input_files: tfrecords_list})\n",
    "            while True:\n",
    "                try:\n",
    "                    # Query_Id, Query_Title_Id, Query_len, Title_len, Query_fre, Title_fre, differQ_T, differQ_T_len, unionT_Q, unionT_Q_len, y = sess.run([query_id, query_title_id, query_len, title_len, fre_query, fre_title, DifferQ_T, DifferQ_T_len, UnionT_Q, UnionT_Q_len, label])\n",
    "                    _, _, Query_len, Title_len, _, _, differQ_T, _, unionT_Q, _, y = sess.run([query_id, query_title_id, query_len, title_len, fre_query, fre_title, DifferQ_T, DifferQ_T_len, UnionT_Q, UnionT_Q_len, label])\n",
    "                    \n",
    "                    p1 = str(differQ_T.decode())\n",
    "                    p2 = str(unionT_Q.decode())\n",
    "                    if(len(p1) != 0):\n",
    "                        p1 = p1[1:-1]\n",
    "                        \n",
    "                    if(len(p2) != 0):\n",
    "                        p2 = p2[1:-1]\n",
    "                    # print(p1,p2,type(p1),type(p2))\n",
    "                    differ_qt.append(p1)\n",
    "                    union_tq.append(p2)\n",
    "                    \n",
    "                    # print(p1,'-'*10,p2)\n",
    "                    # temp = []\n",
    "                    # temp.extend([Query_Id, Query_Title_Id, Query_len, Title_len, Query_fre, Title_fre])\n",
    "                    \n",
    "                    # temp.extend([i for i in differ_q_t[0]])\n",
    "                    # temp.extend([i for i in union_t_q[0]])\n",
    "                    # X.append(temp)\n",
    "                    Y.append(y)\n",
    "                    \n",
    "                    #----------here-----------------\n",
    "                    cnt += 1\n",
    "                    if cnt == batch_size:\n",
    "#                         print(cnt,'------------train\\n')\n",
    "                        cnt = 0\n",
    "                        tokenizer.fit_on_texts(differ_qt)\n",
    "                        differ_list_q = tokenizer.texts_to_sequences(differ_qt)\n",
    "                        differ_q_t = pad_sequences(np.array(differ_list_q), maxlen=maxlen_1)\n",
    "                        \n",
    "                        tokenizer.fit_on_texts(union_tq)\n",
    "                        union_list_q = tokenizer.texts_to_sequences(union_tq)\n",
    "                        union_q_t = pad_sequences(np.array(union_list_q), maxlen=maxlen_1)\n",
    "                        \n",
    "                        p = list(tokenizer.word_index.values())\n",
    "                        # print(p[len(p)-1],'------------train')\n",
    "                        del p\n",
    "                        \n",
    "                        for i in range(batch_size):\n",
    "                            temp_p = list(differ_q_t[i])\n",
    "                            temp_p.extend(list(union_q_t[i]))\n",
    "                            X.append(temp_p)\n",
    "                        yield (np.array(X), np.array(Y))\n",
    "                        X = []\n",
    "                        Y = []\n",
    "                        differ_qt = []\n",
    "                        union_tq = []\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "#             print(cnt,'------------train\\n')\n",
    "            cnt = 0\n",
    "            tokenizer.fit_on_texts(differ_qt)\n",
    "            differ_list_q = tokenizer.texts_to_sequences(differ_qt)\n",
    "            differ_q_t = pad_sequences(np.array(differ_list_q), maxlen=maxlen_1)\n",
    "            \n",
    "            tokenizer.fit_on_texts(union_tq)\n",
    "            union_list_q = tokenizer.texts_to_sequences(union_tq)\n",
    "            union_q_t = pad_sequences(np.array(union_list_q), maxlen=maxlen_1)\n",
    "            \n",
    "            p = list(tokenizer.word_index.values())\n",
    "            # print(p[len(p)-1],'------------train')\n",
    "            del p\n",
    "            \n",
    "            for i in range(len(differ_q_t)):\n",
    "                temp_p = list(differ_q_t[i])\n",
    "                temp_p.extend(list(union_q_t[i]))\n",
    "                X.append(temp_p)\n",
    "            yield (np.array(X), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "C12DBB7A1D9A48A494D87EB65AF8DCDA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_data_from_val_files(path, batch_size):\n",
    "    while True:\n",
    "        tfrecords_list = [os.path.join(dirpath, filename) for dirpath, dirnames, filenames in os.walk(path) \\\n",
    "                                                            for filename in filenames]\n",
    "        input_files = tf.placeholder(tf.string)\n",
    "        dataset = tf.data.TFRecordDataset(input_files)\n",
    "        dataset = dataset.map(parser)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        query_id, query_title_id, query_len, title_len, fre_query, fre_title, DifferQ_T, DifferQ_T_len, UnionT_Q, UnionT_Q_len, label = iterator.get_next()\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        index = 0\n",
    "        instances_num = 0\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        cnt = 0\n",
    "        differ_qt = []\n",
    "        union_tq = []\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(iterator.initializer, feed_dict={input_files: tfrecords_list})\n",
    "            while True:\n",
    "                try:\n",
    "                    # Query_Id, Query_Title_Id, Query_len, Title_len, Query_fre, Title_fre, differQ_T, differQ_T_len, unionT_Q, unionT_Q_len, y = sess.run([query_id, query_title_id, query_len, title_len, fre_query, fre_title, DifferQ_T, DifferQ_T_len, UnionT_Q, UnionT_Q_len, label])\n",
    "                    _, _, Query_len, Title_len, _, _, differQ_T, _, unionT_Q, _, y = sess.run([query_id, query_title_id, query_len, title_len, fre_query, fre_title, DifferQ_T, DifferQ_T_len, UnionT_Q, UnionT_Q_len, label])\n",
    "\n",
    "                    p1 = str(differQ_T.decode())\n",
    "                    p2 = str(unionT_Q.decode())\n",
    "                    if(len(p1) != 0):\n",
    "                        p1 = p1[1:-1]\n",
    "\n",
    "                    if(len(p2) != 0):\n",
    "                        p2 = p2[1:-1]\n",
    "                    # print(p1,p2,type(p1),type(p2))\n",
    "                    differ_qt.append(p1)\n",
    "                    union_tq.append(p2)\n",
    "\n",
    "                    # print(p1,'-'*10,p2)\n",
    "                    # temp = []\n",
    "                    # temp.extend([Query_Id, Query_Title_Id, Query_len, Title_len, Query_fre, Title_fre])\n",
    "\n",
    "                    # temp.extend([i for i in differ_q_t[0]])\n",
    "                    # temp.extend([i for i in union_t_q[0]])\n",
    "                    # X.append(temp)\n",
    "                    Y.append(y)\n",
    "                    cnt += 1\n",
    "                    if cnt == batch_size:\n",
    "#                         print(cnt,'------------val\\n')\n",
    "                        cnt = 0\n",
    "                        tokenizer.fit_on_texts(differ_qt)\n",
    "                        differ_list_q = tokenizer.texts_to_sequences(differ_qt)\n",
    "                        differ_q_t = pad_sequences(np.array(differ_list_q), maxlen=maxlen_1)\n",
    "                        # print(differ_qt, differ_q_t)\n",
    "                        tokenizer.fit_on_texts(union_tq)\n",
    "                        union_list_q = tokenizer.texts_to_sequences(union_tq)\n",
    "                        union_q_t = pad_sequences(np.array(union_list_q), maxlen=maxlen_1)\n",
    "\n",
    "                        p = list(tokenizer.word_index.values())\n",
    "                        # print(p[len(p)-1],'------------val')\n",
    "                        del p\n",
    "\n",
    "                        for i in range(batch_size):\n",
    "                            temp_p = list(differ_q_t[i])\n",
    "                            temp_p.extend(list(union_q_t[i]))\n",
    "                            X.append(temp_p)\n",
    "#                         return np.array(X), np.array(Y)\n",
    "                        yield np.array(X), np.array(Y)\n",
    "                        X = []\n",
    "                        Y = []\n",
    "                        differ_qt = []\n",
    "                        union_tq = []\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            cnt = 0\n",
    "            tokenizer.fit_on_texts(differ_qt)\n",
    "            differ_list_q = tokenizer.texts_to_sequences(differ_qt)\n",
    "            differ_q_t = pad_sequences(np.array(differ_list_q), maxlen=maxlen_1)\n",
    "            \n",
    "            tokenizer.fit_on_texts(union_tq)\n",
    "            union_list_q = tokenizer.texts_to_sequences(union_tq)\n",
    "            union_q_t = pad_sequences(np.array(union_list_q), maxlen=maxlen_1)\n",
    "            \n",
    "            p = list(tokenizer.word_index.values())\n",
    "            # print(p[len(p)-1],'------------train')\n",
    "            del p\n",
    "            \n",
    "            for i in range(len(differ_q_t)):\n",
    "                temp_p = list(differ_q_t[i])\n",
    "                temp_p.extend(list(union_q_t[i]))\n",
    "                X.append(temp_p)\n",
    "            yield (np.array(X), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "8400BEBBB21F4A899F85A191EB18315F",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object read_data_from_val_files at 0x7ff0c00650f8>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-53-ab52c9718097>\", line 90, in read_data_from_val_files\n",
      "  File \"/home/wjw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1587, in __exit__\n",
      "    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\n",
      "  File \"/home/wjw/anaconda3/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/home/wjw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5233, in get_controller\n",
      "    context.context().context_switches.pop()\n",
      "  File \"/home/wjw/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 141, in pop\n",
      "    self.stack.pop()\n",
      "IndexError: pop from empty list\n"
     ]
    }
   ],
   "source": [
    "batch_size_ = 512\n",
    "valGen = read_data_from_val_files(TFRECORDS_VALPATH, batch_size_//2)\n",
    "# x_test, y_test = read_data_from_val_files(TFRECORDS_VALPATH, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "4BB292D5DCB640E3AD1FFF8490A27E3B",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    }
   ],
   "source": [
    "def read_data_from_train_num_files(path):\n",
    "    tfrecords_list = [filenames for dirpath, dirnames, filenames in os.walk(path)]\n",
    "    # print(tfrecords_list)\n",
    "    total = 0\n",
    "    for fn in tfrecords_list[0]:\n",
    "        # print(fn,'-----')\n",
    "        cnt = 0\n",
    "        for record in tf.python_io.tf_record_iterator(TFRECORDS_PATH+'/'+fn):\n",
    "            cnt += 1 \n",
    "        total += cnt\n",
    "#         print(cnt)\n",
    "    return total\n",
    "NUM_TOTAL = read_data_from_train_num_files(TFRECORDS_PATH)\n",
    "print(NUM_TOTAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "B90CE397FF444EB3946EF5A8703FE213",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object read_data_from_train_files at 0x7fef636dd468>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-52-776ca226ac47>\", line 91, in read_data_from_train_files\n",
      "  File \"/home/wjw/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1587, in __exit__\n",
      "    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\n",
      "  File \"/home/wjw/anaconda3/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/home/wjw/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5233, in get_controller\n",
      "    context.context().context_switches.pop()\n",
      "  File \"/home/wjw/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 141, in pop\n",
      "    self.stack.pop()\n",
      "IndexError: pop from empty list\n"
     ]
    }
   ],
   "source": [
    "batch_size_ = 512\n",
    "initial_epoch = 0\n",
    "trainGen = read_data_from_train_files(TFRECORDS_PATH, batch_size_)\n",
    "# trainGen_ = trainGen\n",
    "# num = 0\n",
    "# print(NUM_TOTAL//batch_size_)\n",
    "# while(num <= NUM_TOTAL//batch_size_):\n",
    "#     next(trainGen_)\n",
    "#     num += 1\n",
    "# print(num)\n",
    "# trainGen = read_data_from_files(TFRECORDS_PATH, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hide_input": false,
    "id": "6928D1864A8844708402D06A6E739645",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "36/36 [==============================] - 12s 327ms/step - loss: 0.3230 - auc: 0.8890 - val_loss: 0.9112 - val_auc: 0.5317\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 9s 257ms/step - loss: 0.3226 - auc: 0.8890 - val_loss: 1.1342 - val_auc: 0.5043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0ac486668>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(trainGen, initial_epoch=initial_epoch,steps_per_epoch = math.ceil(NUM_TOTAL/batch_size_), epochs = 2, validation_steps = math.ceil(2000/batch_size_/2),validation_data = valGen, verbose=True)\n",
    "# model.fit(train_X, train_y, batch_size = 512, epochs = 20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4C3D5F849EF24FF885CE8D1D43F45AE8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('model.h5') # HDF5文件, pip install h5py\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73F0863600C84BEE8DB6465385243B49",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model.h5', custom_objects={'auc':auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA39418DCB75449F8BB4C6E06DB9CEE3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = \n",
    "result_data = model.predict(tets_data,batch_size=512, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
